---
title: "ANÁLISIS AVANZADO DE DATOS - Taller 3"
author: "Raúl Andrés Rodriguez - Richard Felipe Bolaños"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: cerulean
    toc: yes
    toc_float:
      collapsed: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)
```

## Librerias

```{r, warning = FALSE }

# Cargar las librerias a utilizar
library(pROC)
library(ggplot2)
library(readxl)
library(mice)
```

## Problema \# 1

Una familia de distribuciones Pθ con θ ∈ Θ pertenece a la familia
exponencial de distribucciones si su fmp/fdp puede escribirse como:

p(x\|η) = h(x)exp(η(θ)t(x) − a(θ))

Para funciones reales h(x), a(θ) y t(x).

Muestre que tanto la distribución bernoulli (utilizada para la regresión
logística), la distribución normal (utilizada en la regresión lineal) y
la distribución Poisson (utilizada en la regresión Poisson sobre
conteos) pertenecen a esta familia de distribuciones.

Cambiando un poco la notación usaremos la notación del libro
**Generalized Linear Models With Examples in R**.

$$
P(y; \theta, \phi) = a(y, \phi) \exp \left( \frac{y\theta - \kappa(\theta)}{\phi} \right)
$$

donde:

-   $\theta$ se llama el **parámetro canónico**.
-   $\kappa(\theta)$ es una función conocida y se llama la **función
    cumulante**.
-   $\phi > 0$ es el **parámetro de dispersión**.
-   $a(y, \phi)$ es una función de normalización que asegura que (5.1)
    es una función de probabilidad. Es decir, $a(y, \phi)$ es la función
    de $\phi$ y $y$ que asegura que $\int P(y; \theta, \phi) \, dy = 1$
    sobre el rango apropiado si $y$ es continua, o que
    $\sum_y P(y; \theta, \phi) , dy = 1$ si $y$ es discreta. La función
    $a(y, \phi)$ no siempre puede ser escrita en forma cerrada.

### Distribución normal

$$
f_Y(y, \theta, \phi) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)
$$ Partiendo de la parte del exponencial de la función de distribución,
aplicamos un poco de algebra:

$$ \exp\left(-\frac{(y - u)^2}{2\sigma^2}\right) \ \exp\left(-\frac{y^2 - 2yu + u^2}{2\sigma^2}\right) \ \exp\left(\frac{yu - \frac{u^2}{2}}{\sigma^2} - \frac{y^2}{2\sigma^2} \right) $$

$$
si $\mu = \theta)$ y $\phi = \sigma^2$, entonces la expresión se simplifica a:
$$

$$
\frac{1}{\sqrt{2\pi \phi^2}}
\exp\left(\frac{y\theta - \frac{\theta^2}{2}}{\phi^2} -
\frac{y^2}{2\phi^2} \right)
$$

Luego aplicando algunas propiedades de la exponencial tenemos:

$$
\frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{y^2}{2\sigma^2}\right) \exp\left(\frac{y\theta -\frac{\theta^2}{2}}{\phi^2} - \frac{y^2}{2\phi^2} \right)
$$

Siendo
$a(y, \phi) =  \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{y^2}{2\sigma^2}\right)$
y $\kappa(\theta)= \frac{\theta^2}{2}$.

### Distribución Poisson

$$
f_Y(y, \theta, \phi) = \frac{\mu^y}{y!} \exp(-\mu)
$$

Aplicamos un poco de algebra descomponemos esto usando propiedades
logarítmicas:

$$
=\log\left(\frac{y^u}{y!} \exp(-u)\right) 
\ =\log\left(\frac{y^u}{y!}\right) + \log\left(\exp(-u)\right) 
\ =y \log(y) - \log(y!) - u 
\ = \exp \left( ylog(u) - u - log(y!) \right ) 
\ = \exp \left( ylog(u) - u \right ) \exp(- log(y!)) 
\ = \exp \left( y log(u) - u \right ) * \frac{1}{y!}
$$

Siendo

$a(y, \phi) =  \frac{1}{y!}$, $\kappa(\theta)= u$ , $\phi=1$ y
$\theta=log(u)$

### Distribución Bernaulli

$$
f_Y(y,\theta,\phi) = \binom{n}{ny} \mu^y (1-\mu)^{n(1-y)}
$$

$$
=\exp\left[ m y \log(\mu) - (m y - m) \log(1 - \mu) + \log\binom{m}{my} \right]\ =\exp\left[ \frac{ y \log\left(\frac{\mu}{1-\mu}\right) - \log\left(\frac{1}{1-\mu}\right)}{1/m} + \log\binom{m}{m y} \right]\ =\binom{m}{m y} \exp\left[ y \theta - \log(1 + e^\theta)  \right]\
$$

Donde

$$\phi = \frac{1}{m} , k(\theta) = \log(1 + e^\theta), \theta = logit(\mu) = \log\left(\frac{\mu}{1-\mu}\right) $ y $ a(y, \phi) = \log\binom{m}{m y}$$

## Problema \# 2

La Universidad de California Irvine (UCI) tiene un repositorio de datos
de ejemplo para el uso de machine learning y aprendizaje estadístico.
Uno de los conjuntos de datos es el denominado Heart Disease, su
descripción detallada se encuentra en la URL a continuación:

<https://archive.ics.uci.edu/ml/datasets/Heart+Disease>

Utilice los datos procesados disponibles en el enlace presentado a
continuación para el desarrollo del ejercicio,

<http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed>.cleveland.data

Con el conjunto de datos completo, construya un modelo de regresión
logístico con función de enlace logit tomando como respuesta la
presencia de la enfermedad cardiaca, use las demás variables como
explicativas en el modelo de regresión. Revise las URL dadas para la
definición de cada una de las variables y note que debe obtener la
variable respuesta categorizando una de las variables del conjunto de
datos. Siga los siguientes pasos en la realización del ejercicio:

### Item \# 1

Imputar datos: El conjunto de datos tiene datos perdidos en algunas
variables. Estos están notados con un ?. Impute los valores perdidos
como la mediana de los datos para las variables corresponientes.

```{r}

# Cargar el dataset
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data", header = FALSE)

# Asignar nombres a las columnas de aceurdo a los metadatos
colnames(data) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "num")

# Reemplazar los valores "?" con NA
data[data == "?"] <- NA

# Imputar los valores perdidos con la mediana
for (col in colnames(data)) {
  if (any(is.na(data[[col]]))) {
    data[[col]][is.na(data[[col]])] <- median(data[[col]], na.rm = TRUE)
  }
}

# Ajustar la variable objetivo num para tener dos valores 0 y 1
data$num <- ifelse(data$num %in% c(1, 2, 3, 4), 1, data$num)

# Verificar si los valores NA han sido imputados correctamente
formatted_summary <- format(summary(data))

# Imprimir el resumen formateado
print (formatted_summary)
```

### Item \# 2

Revisar las distribuciones bivariadas: Revise la distribución de la
variable respuesta para cada una de las covariables categoricas de
manera bivariada. ¿observa algún inconveniente con alguna de las
variables al hacer el análisis?.

```{r}
# Identificar las variables categóricas y la variable respuesta
categorical_variables <- c("sex", "cp", "fbs", "restecg", "exang", "slope", "thal")
response_variable <- "num"

# Realizar análisis bivariado para cada variable categórica
for (variable in categorical_variables) {
  # Crear una tabla de frecuencias cruzadas
  cross_table <- table(data[[variable]], data[[response_variable]])
  print(paste("Distribución bivariada para la variable:", variable))
  print(cross_table)
}
```

Las covariables: **restecg**, **slope** y **thal** tiene pocas
observaciones en una de sus categorias, lo cual puede generar sesgo en
el análisis. Importante destacar que la imputacion en los valores
perdidos ya realizada corrige un posible problema en la precisión del
analisis.

### Item \# 3

Modelo bivariado: Calcule manualmente (como lo vimos en clase, a partir
de la tabla de contingencia), los parámetros estimados de regresión
logística considerando únicamente la variable fbs (glucemia en ayunas) y
la variable respuesta. Verifique el resultado ajustando el glm
correspondiente.

```{r}

# Cargar los datos y calcular la tabla de contingencia
contingency_table <- table(data$fbs, data$num)

# Calcular las probabilidades
probabilities <- apply(contingency_table, 1, function(x) x[2] / sum(x))

# Calcular los odds
log_odds <- log(probabilities / (1 - probabilities))

# Ajustar el modelo de regresión logística 
modelo <- glm(num ~ fbs, data = data, family = binomial)

# Mostrar los resultados
print("Tabla de Contingencia:")
print(contingency_table)
print("Probabilidades:")
print(probabilities)
print("Log Odds:")
print(log_odds)
print("Resumen del Modelo de Regresión Logística:")
summary(modelo)
```

### Item \# 4

Modelo multivariado: Ajuste un nuevo modelo con todas las variables.
¿cuáles variables son significativas mediante el test de Wald? ¿cuáles
no lo son?.

```{r}

# Ajustar el modelo de regresión logística con todas las variables
modelo <- glm(num ~ ., data = data, family = binomial)

# Realizar el test de Wald
wald_test <- summary(modelo)$coefficients[, "Pr(>|z|)"]

# Identificar las variables significativas
# Se define un umbral de significancia = 0.05
umbral_significancia <- 0.05
variables_significativas <- names(wald_test)[wald_test < umbral_significancia]

# Imprimir las variables significativas
print("Variables significativas:")
setdiff(variables_significativas, "(Intercept)")

# Imprimir las variables no significativas
variables_no_significativas <- names(wald_test)[wald_test >= umbral_significancia]
print("Variables no significativas:")
print(variables_no_significativas)
```

### Item \# 5

Visualización de probabilidades predichas bajo modelo multivariado:
Usando el modelo del punto anterior, encuentre las probabilidades de
presentar enfermedad cardiaca y visualicelas junto a la variable
respuesta. ¿Describe el modelo la presencia de enfermedad cardiaca?.

```{r}

# Obtener los datos del modelo
data$Predicted_Prob <- predict(modelo, type = "response")


# Graficar las probalidades
plot(data$num, data$Predicted_Prob, 
     xlab = "Heart Disease", ylab = "Probabilidad Predicha",
     main = "Probabilidades de Enfermedad Cardíaca")

# Agregar una línea horizontal en 0.5 para ayudar a visualizar el umbral de decisión
abline(h = 0.5, lty = 2)

# Agregar puntos rojos para las observaciones con enfermedad cardíaca y puntos azules para las observaciones sin enfermedad cardíaca
points(data$num, data$Predicted_Prob, col = ifelse(data$num == 1, "red", "blue"))

# Agregar leyenda
legend("topright", legend = c("No", "Sí"), pch = 1, col = c("blue", "red"), bty = "n")
```

## Problema \# 3

El conjunto de datos AAD-taller03.xlsx contiene la predicción de
incumplimiento de pago de tarjeta de crédito bajo dos modelos logísticos
diferentes para un total de 9080 clientes. Se cuenta además con la
variable de incumplimiento observada al finalizar el periodo. ¿Cuál de
los dos modelos logísticos tiene mayor poder de predicción? Explique con
fundamento estadístico su resultado.

```{r}

# Cargar datos
AAD_taller03 <- read_excel("AAD-taller03.xlsx")

# Separar predicciones y observaciones
predicciones_modeloA <- AAD_taller03$ScoreLogisticoA 
predicciones_modeloB <- AAD_taller03$ScoreLogisticoB  
observaciones_reales <- AAD_taller03$Incumplimiento


# Calcular el AUC-ROC para cada modelo
roc_obj1 <- roc(observaciones_reales, predicciones_modeloA)
roc_obj2 <- roc(observaciones_reales, predicciones_modeloB)

# Crear dataframes para cada ROC
df_roc1 <- data.frame(tpr = roc_obj1$sensitivities, fpr = 1 - roc_obj1$specificities, model = "Modelo 1")
df_roc2 <- data.frame(tpr = roc_obj2$sensitivities, fpr = 1 - roc_obj2$specificities, model = "Modelo 2")

# Combinar los dataframes
df_roc <- rbind(df_roc1, df_roc2)

# Crear el gráfico ROC con ggplot2
ggplot(df_roc, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  labs(x = "1 - Especificidad", y = "Sensibilidad", title = "Curva ROC") +
  theme_minimal()

# Imprimir los AUC-ROC
print(paste("AUC-ROC del modelo 1: ", auc(roc_obj1)))
print(paste("AUC-ROC del modelo 2: ", auc(roc_obj2)))

```

Dado el resultado de las correspondientes curvas ROC el modelo con mayor
capacidad de predicción es el modelo 1, esto dada la definición de la
curva esta muestra la relación entre la tasa de verdaderos positivos
(sensibilidad) y la tasa de falsos positivos (1 - especificidad) para
diferentes umbrales de clasificación, es decir entre mayor area bajo la
curva se puede establecer un mayor poder de predicción.

## Problema \# 4

Repita el problema 2, pero en lugar de imputar los datos mediante la
mediana en el punto 1, utilice el algoritmo EM.

```{r,warning = FALSE}


# Cargar el dataset
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data", header = FALSE)

# Asignar nombres a las columnas de acuerdo a los metadatos
colnames(data) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "num")

# Reemplazar los valores "?" con NA
data[data == "?"] <- NA

# Imputar los valores perdidos con el algoritmo EM
imputed_data <- mice(data, method = "norm.nob", m = 1, maxit = 50, seed = 500)

completed_data <- complete(imputed_data, 1)

# Ajustar la variable objetivo num para tener dos valores 0 y 1
completed_data$num <- ifelse(completed_data$num %in% c(1, 2, 3, 4), 1, completed_data$num)
```

```{r}
# Ajustar el modelo de regresión logística con todas las variables
modelo <- glm(num ~ ., data = completed_data, family = binomial)

# Realizar el test de Wald
wald_test <- summary(modelo)$coefficients[, "Pr(>|z|)"]

# Identificar las variables significativas
# Se define un umbral de significancia = 0.05
umbral_significancia <- 0.05
variables_significativas <- names(wald_test)[wald_test < umbral_significancia]

# Imprimir las variables significativas
print("Variables significativas:")
setdiff(variables_significativas, "(Intercept)")

# Imprimir las variables no significativas
variables_no_significativas <- names(wald_test)[wald_test >= umbral_significancia]
print("Variables no significativas:")
print(variables_no_significativas)
```

CONCLUSIÓN: No se observa cambios en el modelo, aunque la imputacion EM
utilizada es apropiada para varibles numéricas, y la variable thal es
categórica, lo cual implica que se debe realizar otra tipo de
imputación.
